{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e82e9ac",
   "metadata": {},
   "source": [
    "RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b33b2",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) differ from regular neural networks in how they process information. While standard neural networks pass information in one direction i.e. from input to output, RNNs feed information back into the network at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb04fd7e",
   "metadata": {},
   "source": [
    "Lets understand RNN with a example:\n",
    "\n",
    "Imagine reading a sentence and you try to predict the next word, you don’t rely only on the current word but also remember the words that came before. RNNs work similarly by “remembering” past information and passing the output from one step as input to the next i.e it considers all the earlier words to choose the most likely next word. This memory of previous steps helps the network understand context and make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554dc58",
   "metadata": {},
   "source": [
    "Key Components of RNNs:\n",
    "\n",
    "There are mainly two components of RNNs that we will discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2156bec",
   "metadata": {},
   "source": [
    "1. Recurrent Neurons:\n",
    "\n",
    "The fundamental processing unit in RNN is a Recurrent Unit. They hold a hidden state that maintains information about previous inputs in a sequence. Recurrent units can \"remember\" information from prior steps by feeding back their hidden state, allowing them to capture dependencies across time.\n",
    "\n",
    "2. RNN Unfolding : \n",
    "\n",
    "RNN unfolding or unrolling is the process of expanding the recurrent structure over time steps. During unfolding each step of the sequence is represented as a separate layer in a series illustrating how information flows across each time step.\n",
    "\n",
    "This unrolling enables backpropagation through time (BPTT) a learning process where errors are propagated across time steps to adjust the network’s weights enhancing the RNN’s ability to learn dependencies within sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84433426",
   "metadata": {},
   "source": [
    "Recurrent Neural Network Architecture\n",
    "RNNs share similarities in input and output structures with other deep learning architectures but differ significantly in how information flows from input to output. Unlike traditional deep neural networks where each dense layer has distinct weight matrices. RNNs use shared weights across time steps, allowing them to remember information over sequences.\n",
    "\n",
    "In RNNs the hidden state H(i)\n",
    "​\n",
    " ​ is calculated for every input X(i)\n",
    " ​ to retain sequential dependencies. The computations follow these core formulas:\n",
    "\n",
    " 1. Hidden State Calculation:\n",
    "     h=σ(U⋅X+W⋅h(t−1)+B) \n",
    "\n",
    "     Here:\n",
    "     \n",
    "        1. h represents the current hidden state.\n",
    "\n",
    "        2. U and W are weight matrices.\n",
    "\n",
    "        3. X is the current input\n",
    "\n",
    "        4. B is the bias term\n",
    "\n",
    "        5. σ is the activaton function \n",
    "\n",
    "        6. h(t-1) is the previous hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e25f1",
   "metadata": {},
   "source": [
    "2. Output Calculation:\n",
    "\n",
    "         Y=O(V⋅h+C)\n",
    "\n",
    "         The output Y is calculated by applying O an activation function to the weighted hidden state where V and C represent weights and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b4cf0",
   "metadata": {},
   "source": [
    "3. Overall Function:\n",
    "\n",
    "              Y=f(X,h,W,U,V,B,C) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7150b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
